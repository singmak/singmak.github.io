---
title: "The road to Zero Downtime: Migration of data and database schema changes"
date: "2022-03-05"
description: "How can we make changes to the database schema and migrate data without any downtime in the server?"
tags: ['database', 'deployment']
---

Recently, our team is discussing if it's possible to deploy changes to production without downtime. Some of the difficult tasks to do when the server is up and running are:
* Applying database schema changes
* Data migration

While it may sound risky to make changes to database schema or do data migration when the clients are still making requests and producing new data transactions it's not impossible to handle these tasks without bringing down the service. In this post, I am going to discuss how we can achieve no downtime in our service even when we need to apply new data schema changes and perform data migration.
## Schema changes

To achieve no downtime, we should deploy database schema changes before deploying the new version of the API service. The reason is that if we deploy the new version of the API service first, the APIs that require the new database schema changes will be broken since the new database schema changes are not there yet.

Therefore, the new version of the database schema should be deployed first and it must be compatible with the existing version of the API in the production environment when the new API version is still not deployed, which means we cannot simply remove or rename any existing tables and columns or change the column data types. We can only do those changes (delete or alter tables, columns, etc) only when we are sure that the tables or the table columns are not referenced anywhere anymore in the API service.

## Data migration

Data migration may be required for different reasons such as infrastructure changes or business logic changes. It can be tricky to achieve no downtime in doing data migration since new data can still be created during the data migration. Therefore, during data migration, we need to handle it at the API codes level so that the new data are also automatically migrated. 

For example, assume that there is an existing API service, which contains APIs that perform CRUD in a database table A. There is also a web application that makes requests to the API service.

We need to implement a new business flow and it will replace the existing flow. This new business flow requires the data to be structured differently so a new table B is created to meet the requirement of the new data model. A data migration script is written to transform the data in table A into the new structure and store the transformed data in the new Table B. Both the API service and the web application also need to be updated to implement the new business flow.

How to implement the API in the API services and perform the deployment in such a way that we can achieve no downtime?

First of all, even if we have deployed a new version of the web application, we cannot be sure that all client browsers will be loading the latest version of the web application since we cannot know when the users' browsers will load the latest version of the web application. So for a short amount of time, there will be some users who are still interacting with the old version of the web application. Therefore, in addition to adding new APIs for the new business flow, the existing API that is being used to write data to Table A also need to be updated to be able to transform the data and write to the new Table B. Doing it this way, we will only need to run the data migration script once to migrate the existing data.

After the new version of the API service is deployed, we can run the migration script to migrate the data from Table A to Table B with the new data structure. During this time, the web application is still not updated and it is still making requests to the old API. But since the old API has been updated to also transform and write data to Table B, we don't need to worry about missing some data in the new Table B.

After the data migration script finishes migrating the data, we can safely deploy the latest version of the web application, that is updated for the new business flow, and makes requests to the new API.

In summary, to achieve no downtime for data migration, I think the deployment order should be like this:
1. Apply data schema changes to production database (make sure it's backward compatible with current API service)
2. Deploy API service (make sure it's backward compatible with current web application and also automatically handle data migration for new data)
3. Run data migration script to migrate existing data
4. Deploy the web application

After running with the new updated version of the API service for some time and when all users are using the new version of the web application, the old APIs and the old Table A will eventually become obsolete and we can remove them in the next release of the API service.

![data-migration](./images/data-migration.png)

